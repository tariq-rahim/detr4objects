{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82c08bab",
   "metadata": {},
   "source": [
    "### Object Dectection Using DETR ( Transformer Based )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd4b09a",
   "metadata": {},
   "source": [
    "Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4f4364",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torchvision\n",
    "from transformers import DetrImageProcessor\n",
    "import os\n",
    "import supervision as sv\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from transformers import DetrForObjectDetection\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b093164",
   "metadata": {},
   "source": [
    "Loading image data with a custom Dataset ( i.e. Coco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279b31db",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "dataset = r'custom detr\\dataset_path' # Replace with your dataset path\n",
    "\n",
    "ANNOTATION_FILE_NAME = \"annotation_file.json\"\n",
    "TRAIN_DIRECTORY = os.path.join(dataset, \"train\")\n",
    "VAL_DIRECTORY = os.path.join(dataset, \"valid\")\n",
    "TEST_DIRECTORY = os.path.join(dataset, \"test\")\n",
    "\n",
    "\n",
    "class CocoDetection(torchvision.datasets.CocoDetection):\n",
    "    def __init__(\n",
    "        self, \n",
    "        image_directory_path: str, \n",
    "        image_processor, \n",
    "        train: bool = True\n",
    "    ):\n",
    "        annotation_file_path = os.path.join(image_directory_path, ANNOTATION_FILE_NAME)\n",
    "        super(CocoDetection, self).__init__(image_directory_path, annotation_file_path)\n",
    "        self.image_processor = image_processor\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        images, annotations = super(CocoDetection, self).__getitem__(idx)        \n",
    "        image_id = self.ids[idx]\n",
    "        annotations = {'image_id': image_id, 'annotations': annotations}\n",
    "        encoding = self.image_processor(images=images, annotations=annotations, return_tensors=\"pt\")\n",
    "        pixel_values = encoding[\"pixel_values\"].squeeze()\n",
    "        target = encoding[\"labels\"][0]\n",
    "\n",
    "        return pixel_values, target\n",
    "\n",
    "\n",
    "TRAIN_DATASET = CocoDetection(image_directory_path=TRAIN_DIRECTORY, image_processor=image_processor, train=True)\n",
    "VAL_DATASET = CocoDetection(image_directory_path=VAL_DIRECTORY, image_processor=image_processor, train=False)\n",
    "TEST_DATASET = CocoDetection(image_directory_path=TEST_DIRECTORY, image_processor=image_processor, train=False)\n",
    "\n",
    "print(\"Number of training examples:\", len(TRAIN_DATASET))\n",
    "print(\"Number of validation examples:\", len(VAL_DATASET))\n",
    "print(\"Number of test examples:\", len(TEST_DATASET))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcb9186",
   "metadata": {},
   "source": [
    "Find Number of Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fae5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = TRAIN_DATASET.coco.cats\n",
    "id2label = {k: v['name'] for k,v in categories.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ae07cb",
   "metadata": {},
   "source": [
    "Visualize an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7711d267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get random image ID from COCO dataset\n",
    "image_ids = TRAIN_DATASET.coco.getImgIds()\n",
    "image_id = random.choice(image_ids)\n",
    "print(f'Image #{image_id}')\n",
    "\n",
    "# Load image info and annotations\n",
    "image_info = TRAIN_DATASET.coco.loadImgs(image_id)[0]\n",
    "annotations = TRAIN_DATASET.coco.imgToAnns[image_id]\n",
    "\n",
    "# Load image using OpenCV\n",
    "image_path = os.path.join(TRAIN_DATASET.root, image_info['file_name'])\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "# Convert COCO bbox to xyxy and get class IDs\n",
    "xyxy = []\n",
    "class_ids = []\n",
    "\n",
    "for ann in annotations:\n",
    "    x, y, w, h = ann['bbox']\n",
    "    xyxy.append([x, y, x + w, y + h])  # COCO format is [x, y, width, height]\n",
    "    class_ids.append(ann['category_id'])\n",
    "\n",
    "# Build class ID to label mapping\n",
    "categories = TRAIN_DATASET.coco.cats\n",
    "id2label = {k: v['name'] for k, v in categories.items()}\n",
    "labels = [id2label[class_id] for class_id in class_ids]\n",
    "\n",
    "# Create Detections object with labels\n",
    "detections = sv.Detections(\n",
    "    xyxy=np.array(xyxy),\n",
    "    class_id=np.array(class_ids),\n",
    "    data={\"labels\": labels}\n",
    ")\n",
    "\n",
    "# Annotate image with bounding boxes and labels\n",
    "box_annotator = sv.BoxAnnotator()\n",
    "annotated_image = box_annotator.annotate(scene=image.copy(), detections=detections)\n",
    "\n",
    "# Convert BGR to RGB for display with matplotlib\n",
    "image_rgb = cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Show image\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(image_rgb)\n",
    "plt.axis('off')\n",
    "plt.title(f\"Image #{image_id}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580812f9",
   "metadata": {},
   "source": [
    "Turn custom loaded images into DataLoader's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f63eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    pixel_values = [item[0] for item in batch]\n",
    "    encoding = image_processor.pad(pixel_values, return_tensors=\"pt\")\n",
    "    labels = [item[1] for item in batch]\n",
    "    return {\n",
    "        'pixel_values': encoding['pixel_values'],\n",
    "        'pixel_mask': encoding['pixel_mask'],\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "TRAIN_DATALOADER = DataLoader(dataset=TRAIN_DATASET, collate_fn=collate_fn, batch_size=4, shuffle=True)\n",
    "VAL_DATALOADER = DataLoader(dataset=VAL_DATASET, collate_fn=collate_fn, batch_size=4)\n",
    "TEST_DATALOADER = DataLoader(dataset=TEST_DATASET, collate_fn=collate_fn, batch_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a56f7ac",
   "metadata": {},
   "source": [
    "Model: DETR directly predicts (in parallel) the final set of detections by combining a common CNN with a Transformer architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f674326",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Detr(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, lr, lr_backbone, weight_decay):\n",
    "        super().__init__()\n",
    "        self.model = DetrForObjectDetection.from_pretrained(\n",
    "            pretrained_model_name_or_path=\"facebook/detr-resnet-50\", \n",
    "            num_labels=len(id2label),\n",
    "            ignore_mismatched_sizes=True\n",
    "        )\n",
    "        \n",
    "        self.lr = lr\n",
    "        self.lr_backbone = lr_backbone\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "    def forward(self, pixel_values, pixel_mask):\n",
    "        return self.model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n",
    "\n",
    "    def common_step(self, batch, batch_idx):\n",
    "        pixel_values = batch[\"pixel_values\"]\n",
    "        pixel_mask = batch[\"pixel_mask\"]\n",
    "        labels = [{k: v.to(self.device) for k, v in t.items()} for t in batch[\"labels\"]]\n",
    "\n",
    "        outputs = self.model(pixel_values=pixel_values, pixel_mask=pixel_mask, labels=labels)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        loss_dict = outputs.loss_dict\n",
    "\n",
    "        return loss, loss_dict\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, loss_dict = self.common_step(batch, batch_idx)     \n",
    "        # logs metrics for each training_step, and the average across the epoch\n",
    "        self.log(\"training_loss\", loss)\n",
    "        for k,v in loss_dict.items():\n",
    "            self.log(\"train_\" + k, v.item())\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, loss_dict = self.common_step(batch, batch_idx)     \n",
    "        self.log(\"validation/loss\", loss)\n",
    "        for k, v in loss_dict.items():\n",
    "            self.log(\"validation_\" + k, v.item())\n",
    "            \n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        param_dicts = [\n",
    "            {\n",
    "                \"params\": [p for n, p in self.named_parameters() if \"backbone\" not in n and p.requires_grad]},\n",
    "            {\n",
    "                \"params\": [p for n, p in self.named_parameters() if \"backbone\" in n and p.requires_grad],\n",
    "                \"lr\": self.lr_backbone,\n",
    "            },\n",
    "        ]\n",
    "        return torch.optim.AdamW(param_dicts, lr=self.lr, weight_decay=self.weight_decay)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return TRAIN_DATALOADER\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return VAL_DATALOADER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5bfda8",
   "metadata": {},
   "source": [
    "Initializes a DETR model and runs a training batch through it to get object detection predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c7231e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Detr(lr=1e-4, lr_backbone=1e-5, weight_decay=1e-4)\n",
    "\n",
    "batch = next(iter(TRAIN_DATALOADER))\n",
    "outputs = model(pixel_values=batch['pixel_values'], pixel_mask=batch['pixel_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f76895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import DetrForObjectDetection\n",
    "# import torch\n",
    "\n",
    "# # Instantiate model correctly (NOT using meta device)\n",
    "# model = DetrForObjectDetection.from_pretrained(\n",
    "#     \"facebook/detr-resnet-50\",  # or your fine-tuned checkpoint\n",
    "# )\n",
    "\n",
    "# # Put model on correct device\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "# model.train()  # if you're training\n",
    "\n",
    "# # Get batch\n",
    "# batch = next(iter(TRAIN_DATALOADER))\n",
    "# pixel_values = batch['pixel_values'].to(device)\n",
    "# pixel_mask = batch['pixel_mask'].to(device)\n",
    "\n",
    "# # Forward pass\n",
    "# outputs = model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b701a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('medium')  # Enable Tensor Cores for speed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4db375",
   "metadata": {},
   "source": [
    "Set up a PyTorch Lightning Trainer to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3d7b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "\n",
    "MAX_EPOCHS = 100\n",
    "\n",
    "trainer = Trainer( max_epochs=MAX_EPOCHS, gradient_clip_val=0.1, accumulate_grad_batches=8, log_every_n_steps=5)\n",
    "\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096c1e45",
   "metadata": {},
   "source": [
    "Save trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070d84d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = 'custom-model'\n",
    "model.model.save_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90a864c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DetrImageProcessor\n",
    "\n",
    "# Load base processor (from pretrained or your original)\n",
    "processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "\n",
    "processor.save_pretrained(\"custom-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ccc029",
   "metadata": {},
   "source": [
    "Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65803a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading model\n",
    "import torch\n",
    "MODEL_PATH = 'custom-model'\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = DetrForObjectDetection.from_pretrained(MODEL_PATH)\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1ffc7b",
   "metadata": {},
   "source": [
    "Generate random predictions using the trained model, evaluate the results, and save the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871aaad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision.ops import nms\n",
    "from transformers import DetrForObjectDetection, DetrImageProcessor\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# === Configuration ===\n",
    "MODEL_PATH = 'custom-model'\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "IMAGE_DIR = r'dataset_path\\test'  # Replace with your test images directory\n",
    "OUTPUT_DIR = os.path.join(IMAGE_DIR, \"output\")\n",
    "SCORE_THRESHOLD = 0.5\n",
    "NMS_THRESHOLD = 0.3\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# === Load COCO-style label map from TEST_DATASET ===\n",
    "categories = TEST_DATASET.coco.cats\n",
    "id2label = {k: v['name'] for k, v in categories.items()}\n",
    "\n",
    "# === Load model and processor ===\n",
    "processor = DetrImageProcessor.from_pretrained(MODEL_PATH)\n",
    "model = DetrForObjectDetection.from_pretrained(MODEL_PATH)\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "# === Get all image files in the directory ===\n",
    "image_extensions = ('.jpg', '.jpeg', '.png', '.bmp', '.tif', '.tiff')\n",
    "image_files = [f for f in os.listdir(IMAGE_DIR) if f.lower().endswith(image_extensions)]\n",
    "\n",
    "if not image_files:\n",
    "    print(f\"No images found in {IMAGE_DIR}\")\n",
    "else:\n",
    "    for img_name in image_files:\n",
    "        img_path = os.path.join(IMAGE_DIR, img_name)\n",
    "        print(f\"\\nProcessing: {img_name}\")\n",
    "\n",
    "        # === Load and preprocess image ===\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        inputs = processor(images=image, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "        # === Inference ===\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        # === Post-processing ===\n",
    "        results = processor.post_process_object_detection(outputs, target_sizes=[image.size[::-1]], threshold=SCORE_THRESHOLD)\n",
    "        result = results[0]\n",
    "\n",
    "        if len(result[\"scores\"]) == 0:\n",
    "            print(\"No objects detected.\")\n",
    "            continue\n",
    "\n",
    "        # === Extract predictions ===\n",
    "        boxes = result[\"boxes\"]\n",
    "        scores = result[\"scores\"]\n",
    "        labels = result[\"labels\"]\n",
    "\n",
    "        # === Apply NMS ===\n",
    "        keep_indices = nms(boxes, scores, NMS_THRESHOLD)\n",
    "        boxes = boxes[keep_indices]\n",
    "        scores = scores[keep_indices]\n",
    "        labels = labels[keep_indices]\n",
    "\n",
    "        # === Print detections ===\n",
    "        num_labels = len(id2label)\n",
    "        colors = plt.colormaps['tab20'].resampled(num_labels)\n",
    "\n",
    "        for score, label, box in zip(scores, labels, boxes):\n",
    "            label_name = id2label.get(label.item(), f\"Label_{label.item()}\")\n",
    "            print(f\"Label: {label_name}, Score: {score.item():.3f}, Box: {box.tolist()}\")\n",
    "\n",
    "       # === Visualization (Corrected) ===\n",
    "        # Get image dimensions (in pixels)\n",
    "        img_width, img_height = image.size\n",
    "        dpi = 100  # Choose a DPI value\n",
    "        figsize = (img_width / dpi, img_height / dpi)\n",
    "\n",
    "        # Create figure matching the image size\n",
    "        fig, ax = plt.subplots(1, figsize=figsize, dpi=dpi)\n",
    "        ax.imshow(image)\n",
    "        ax.set_axis_off()  # Remove axes completely\n",
    "\n",
    "        # Draw bounding boxes and labels\n",
    "        for score, label, box in zip(scores, labels, boxes):\n",
    "            xmin = box[0].cpu().item()\n",
    "            ymin = box[1].cpu().item()\n",
    "            xmax = box[2].cpu().item()\n",
    "            ymax = box[3].cpu().item()\n",
    "\n",
    "            width, height = xmax - xmin, ymax - ymin\n",
    "            label_name = id2label.get(label.item(), f\"Label_{label.item()}\")\n",
    "            color = colors(label.item())\n",
    "\n",
    "            rect = patches.Rectangle((xmin, ymin), width, height, linewidth=2,\n",
    "                                    edgecolor=color, facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "            ax.text(xmin, ymin - 10, f'{label_name}: {score:.2f}',\n",
    "                    fontsize=12, color='white',\n",
    "                    bbox=dict(facecolor=color, alpha=0.7, pad=2))\n",
    "\n",
    "        # === Save output image (no whitespace) ===\n",
    "        output_img_path = os.path.join(OUTPUT_DIR, f\"{os.path.splitext(img_name)[0]}_pred.jpg\")\n",
    "        fig.savefig(output_img_path, dpi=dpi, bbox_inches='tight', pad_inches=0)\n",
    "        plt.close(fig)\n",
    "        print(f\"Saved to: {output_img_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff970751",
   "metadata": {},
   "source": [
    "Raw Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc1a5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import torch\n",
    "# from torchvision.ops import nms\n",
    "# from transformers import DetrForObjectDetection, DetrImageProcessor\n",
    "# from PIL import Image\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.patches as patches\n",
    "\n",
    "# # === Configuration ===\n",
    "# MODEL_PATH = 'custom-model'\n",
    "# DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# IMAGE_DIR = r'dataset_path\\test'  # Replace with your test images directory\n",
    "# SCORE_THRESHOLD = 0.5\n",
    "# NMS_THRESHOLD = 0.3\n",
    "\n",
    "# # === Load COCO-style label map from TEST_DATASET ===\n",
    "# categories = TEST_DATASET.coco.cats\n",
    "# id2label = {k: v['name'] for k, v in categories.items()}\n",
    "\n",
    "# # === Load model and processor ===\n",
    "# processor = DetrImageProcessor.from_pretrained(MODEL_PATH)\n",
    "# model = DetrForObjectDetection.from_pretrained(MODEL_PATH)\n",
    "# model.to(DEVICE)\n",
    "# model.eval()\n",
    "\n",
    "# # === Get all image files in the directory ===\n",
    "# image_extensions = ('.jpg', '.jpeg', '.png', '.bmp', '.tif', '.tiff')\n",
    "# image_files = [f for f in os.listdir(IMAGE_DIR) if f.lower().endswith(image_extensions)]\n",
    "\n",
    "# if not image_files:\n",
    "#     print(f\"No images found in {IMAGE_DIR}\")\n",
    "# else:\n",
    "#     for img_name in image_files:\n",
    "#         img_path = os.path.join(IMAGE_DIR, img_name)\n",
    "#         print(f\"\\nProcessing: {img_name}\")\n",
    "\n",
    "#         # === Load and preprocess image ===\n",
    "#         image = Image.open(img_path).convert(\"RGB\")\n",
    "#         inputs = processor(images=image, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "#         # === Inference ===\n",
    "#         with torch.no_grad():\n",
    "#             outputs = model(**inputs)\n",
    "\n",
    "#         # === Post-processing ===\n",
    "#         results = processor.post_process_object_detection(outputs, target_sizes=[image.size[::-1]], threshold=SCORE_THRESHOLD)\n",
    "#         result = results[0]\n",
    "\n",
    "#         if len(result[\"scores\"]) == 0:\n",
    "#             print(\"No objects detected.\")\n",
    "#             continue\n",
    "\n",
    "#         # === Extract predictions ===\n",
    "#         boxes = result[\"boxes\"]\n",
    "#         scores = result[\"scores\"]\n",
    "#         labels = result[\"labels\"]\n",
    "\n",
    "#         # === Apply NMS ===\n",
    "#         keep_indices = nms(boxes, scores, NMS_THRESHOLD)\n",
    "#         boxes = boxes[keep_indices]\n",
    "#         scores = scores[keep_indices]\n",
    "#         labels = labels[keep_indices]\n",
    "\n",
    "#         # === Print detections ===\n",
    "#         num_labels = len(id2label)\n",
    "#         colors = plt.colormaps['tab20'].resampled(num_labels)\n",
    "\n",
    "#         for score, label, box in zip(scores, labels, boxes):\n",
    "#             label_name = id2label.get(label.item(), f\"Label_{label.item()}\")\n",
    "#             print(f\"Label: {label_name}, Score: {score.item():.3f}, Box: {box.tolist()}\")\n",
    "\n",
    "#         # === Visualization ===\n",
    "#         fig, ax = plt.subplots(1, figsize=(12, 8))\n",
    "#         ax.imshow(image)\n",
    "\n",
    "#         for score, label, box in zip(scores, labels, boxes):\n",
    "#             xmin = box[0].cpu().item()\n",
    "#             ymin = box[1].cpu().item()\n",
    "#             xmax = box[2].cpu().item()\n",
    "#             ymax = box[3].cpu().item()\n",
    "\n",
    "#             width, height = xmax - xmin, ymax - ymin\n",
    "#             label_name = id2label.get(label.item(), f\"Label_{label.item()}\")\n",
    "#             color = colors(label.item())\n",
    "\n",
    "#             rect = patches.Rectangle((xmin, ymin), width, height, linewidth=2,\n",
    "#                                      edgecolor=color, facecolor='none')\n",
    "#             ax.add_patch(rect)\n",
    "\n",
    "#             ax.text(xmin, ymin - 10, f'{label_name}: {score:.2f}',\n",
    "#                     fontsize=12, color='white', bbox=dict(facecolor=color, alpha=0.7, pad=2))\n",
    "\n",
    "#         # plt.axis('off')\n",
    "#         # plt.tight_layout()\n",
    "#         # plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dll",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
